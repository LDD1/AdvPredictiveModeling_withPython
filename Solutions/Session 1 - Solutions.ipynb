{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://datascience.uci.edu/wp-content/uploads/sites/2/2014/09/data_science_logo_with_image1.png 'UCI_data_science')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Predictive Modeling with Python \n",
    "## Session #2: Ensembles of Classifiers\n",
    "Author: [Eric Nalisnick](http://www.ics.uci.edu/~enalisni/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schedule for Today\n",
    "\n",
    "|Start Time | Session |\n",
    "|-----------|---------|\n",
    "|8:30am     | Check In|\n",
    "|9:00am     | **Feature Engineering** |\n",
    "|10:30am    | Coffee & Bagels|\n",
    "|10:45am    | **Ensembling** |\n",
    "|12:30pm    | End|\n",
    "\n",
    "### Goals of this Lesson\n",
    "- Feature transformations\n",
    "    - Std. Normal Scaling\n",
    "    - Log Transform\n",
    "    - Domain-Specific Transform\n",
    "    \n",
    "- Principal Component Analysis (PCA)\n",
    "    - Model and Learning\n",
    "    - PCA for Images\n",
    "    - PCA for Visualization\n",
    "\n",
    "### References \n",
    "- Chapter 14 of [*Elements of Statistical Learning* by Hastie, Tibshirani, Friedman](http://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf)\n",
    "- [A Few Useful Things to Know about Machine Learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf)\n",
    "- [SciKit-Learn's documentation on data preprocessing](http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)\n",
    "- [SciKit-Learn's documentation on dimensionality reduction](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)\n",
    "\n",
    "## 0.  Preliminaries\n",
    "First we need to import Numpy, Pandas, MatPlotLib..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import cPickle as cp\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we need functions for shuffling the data and calculating classification errrors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### function for shuffling the data and labels\n",
    "def shuffle_in_unison(features, labels):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(features)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(labels)\n",
    "    \n",
    "### calculate classification errors\n",
    "# return a percentage: (number misclassified)/(total number of datapoints)\n",
    "def calc_classification_error(predictions, class_labels):\n",
    "    n = predictions.size\n",
    "    num_of_errors = 0.\n",
    "    for idx in xrange(n):\n",
    "        if (predictions[idx] >= 0.5 and class_labels[idx]==0) or (predictions[idx] < 0.5 and class_labels[idx]==1):\n",
    "            num_of_errors += 1\n",
    "    return num_of_errors/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1  Load the dataset of paintings\n",
    "We are going to use the Bob Ross paintings dataset throughout this session.  Let's load the PCA'd version we saved during the first session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# load the 403 x 360,000 matrix\n",
    "br_paintings = np.load(open('../data/bob_ross/bob_ross_paintings.npy','rb'))\n",
    "\n",
    "# perform PCA again\n",
    "pca = PCA(n_components=400)\n",
    "start_time = time.time()\n",
    "pca_paintings = pca.fit_transform(br_paintings)\n",
    "end_time = time.time()\n",
    "\n",
    "# remove the br_paintings from memory\n",
    "br_paintings = None\n",
    "\n",
    "print \"Training took a total of %.2f seconds.\" %(end_time-start_time)\n",
    "print \"Preserved percentage of original variance: %.2f%%\" %(pca.explained_variance_ratio_.sum() * 100) \n",
    "print \"Dataset is now of size: %d x %d\"%(pca_paintings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to formulate a binary classification problem.  In the data folder there's a file that has labels denoting what is in each painting (tree, mountain, etc.).  Let's load it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "br_labels_data = pd.read_csv('../data/bob_ross/elements-by-episode.csv')\n",
    "br_labels_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's make the two classes 1 = 'painting contains hill or mountain', 0 = 'doesn't contain hill/mountain': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels = (br_labels_data['HILLS'] + br_labels_data['MOUNTAIN'] + br_labels_data['MOUNTAINS'] + br_labels_data['SNOWY_MOUNTAIN'] > 0).astype('int8').as_matrix()\n",
    "print \"Contains mountain?: \"+str(bool(labels[5]))\n",
    "recon_img = pca.inverse_transform(pca_paintings[5,:])\n",
    "plt.imshow(np.reshape(recon_img, (300, 400, 3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Make training and test split..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set the random number generator for reproducability\n",
    "np.random.seed(182)\n",
    "\n",
    "# shuffle data\n",
    "N = pca_paintings.shape[0]\n",
    "shuffle_in_unison(pca_paintings, labels)\n",
    "\n",
    "# split into train and test sets\n",
    "train_features = pca_paintings[:int(.8*N), :]\n",
    "test_features = pca_paintings[int(.8*N):, :]\n",
    "train_labels = labels[:int(.8*N)]\n",
    "test_labels = labels[int(.8*N):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 0.2  Run a baseline classifier\n",
    "In order to see the improvements that ensembling provides, let's train a baseline logistic regression classifier for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(182)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# initialize and train a logistic regression model\n",
    "lr_model = LogisticRegression()\n",
    "lr_model.fit(train_features, train_labels)\n",
    "\n",
    "# compute error on training data\n",
    "lr_predictions = lr_model.predict(test_features)\n",
    "one_model_test_error_rate = calc_classification_error(lr_predictions, test_labels)\n",
    "\n",
    "print \"Classification error on test set: %.2f%%\" %(one_model_test_error_rate*100)\n",
    "# compute the baseline error since the classes are imbalanced\n",
    "print \"Baseline Error: %.2f%%\" %((sum(test_labels)*100.)/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  The Bias-Variance Tradeoff\n",
    "\n",
    "When faced with an important decision, its common to ask multiple people for their advice.  Why should a classification decision be any different?  If computer power is not a limiting factor--which is usually the case--why not train multiple classifiers and combine their predictions?  This is exactly what *ensembling* classifiers does.  In this section we'll cover three methods for combining classifiers: bagging, averaging, and stacking.  But first, let's examine why one classifier is usually not enough.  It can be formalized as a tradeoff between *bias* and *variance*.  \n",
    "\n",
    "Recall the squared loss function: $$\\mathcal{L} = \\sum_{i}^{N} (y_{i} - f(\\mathbf{x}_{i}))^{2}. $$  This loss is over a particular training set {$\\mathbf{X}, \\mathbf{y}$} but we are really interested in the loss over all possible datasets we could have observed, $\\{\\mathbf{X}, \\mathbf{y}\\} \\sim p(\\mathcal{D})$: $$\\mathbb{E}_{p(\\mathcal{D})}[\\mathcal{L}] = \\mathbb{E}_{p(\\mathcal{D})}[(y_{i} - f(\\mathbf{x}_{i}))^{2}]. $$  After some [algebraic manipulations](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff#Derivation), we can re-write the expected loss as $$\\mathbb{E}_{p(\\mathcal{D})}[\\mathcal{L}] = (f^{*}(\\mathbf{x}_{i}) - \\mathbb{E}[f(\\mathbf{x}_{i})])^{2} + \\text{Var}[f(\\mathbf{x}_{i})] + \\text{constant (error)}. $$  The first term, $(f^{*}(\\mathbf{x}_{i}) - \\mathbb{E}[f(\\mathbf{x}_{i})])^{2}$, is the squared difference between the expected value of the classifier $f$ and the **perfect, true** classifier $f^{*}$.  This difference is known as the *bias* of a classifier.  For instance, a linear model has a strong bias since its functional form is rather simple (unless the optimial classifier is also a linear function).  The second term, $\\text{Var}[f(\\mathbf{x}_{i})]$, is the variance of our classifier.  Basically, this term captures the variability in outputs.  The main point is that if a classifier has *low* bias, meaning it is a very powerful function, then it will usually have high *variance* since this power allows it to generate a wide range of outputs.  And vice versa.  What I just said can be represented graphically as \n",
    "![bias_variance_pic](../graphics/bias-variance.png)\n",
    "Ensembling classifiers all but always produces better performance because it **reduces variance without incurring bias**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Bootstrap Aggregating ('Bagging')\n",
    "In section 1, when I mentioned using multiple classifiers, you probably thought I was talking about training and combining several different kinds of classifiers.  We will do that.  But first we'll discuss something simpler: training the same classifier on multiple datasets.  \n",
    "\n",
    "### 2.1 Bootstrap Resampling\n",
    "We can squeeze 'extra robustness' out of our dataset by doing the following simple procedure.  Whereas our original dataset is {}\n",
    "![bootstrap_diagram](../graphics/bootstrap_graphic.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### function for bootstrap resampling\n",
    "def bootstrap_resample(features, labels, n_resamples):\n",
    "    N = features.shape[0]\n",
    "    idxs = np.arange(N)\n",
    "    resampled_idxs = np.random.choice(idxs, size=(N,n_resamples), replace=True)\n",
    "    boot_samps_x = []\n",
    "    boot_samps_y = []\n",
    "    for i in xrange(n_resamples):\n",
    "        boot_samps_x.append(features[resampled_idxs[:,i],:])\n",
    "        boot_samps_y.append(labels[resampled_idxs[:,i]])\n",
    "    return boot_samps_x, boot_samps_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training on Bootstrap Samples\n",
    "We can squeeze 'extra robustness' out of our dataset by doing the following simple procedure.  Whereas our original dataset is {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_and_predict_on_bootstrap_samples(model, bootstrapped_features, bootstrapped_labels, test_features, n_bootstrap_samples):\n",
    "    n_test = test_features.shape[0]\n",
    "    ensemb_probs = np.zeros((n_test,))\n",
    "    ensemb_preds = np.zeros((n_test,))\n",
    "    for idx in xrange(n_bootstrap_samples):\n",
    "        print \"training model #%d\" %(idx+1)\n",
    "        model.fit(bootstrapped_features[idx], bootstrapped_labels[idx])\n",
    "        ensemb_probs += model.predict_proba(test_features)[:,1]\n",
    "        ensemb_preds += model.predict(test_features)\n",
    "    ensemb_probs /= n_bootstrap_samples\n",
    "    ensemb_preds /= n_bootstrap_samples\n",
    "    ensemb_probs = np.around(ensemb_probs)\n",
    "    ensemb_preds = np.around(ensemb_preds)\n",
    "    return ensemb_probs, ensemb_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(182)\n",
    "\n",
    "n_bootstrap_samples = 10\n",
    "bootstrapped_features, bootstrapped_labels = bootstrap_resample(train_features, train_labels, n_bootstrap_samples)\n",
    "\n",
    "ensembled_probs, ensembled_preds = fit_and_predict_on_bootstrap_samples(lr_model, bootstrapped_features, bootstrapped_labels, test_features, n_bootstrap_samples)\n",
    "\n",
    "print\n",
    "print \"Averaging probabilities: classification error on test set is %.2f%%\" %(calc_classification_error(ensembled_probs, test_labels)*100)\n",
    "print \"Averaging predictions: classification error on test set is %.2f%%\" %(calc_classification_error(ensembled_preds, test_labels)*100)\n",
    "print\n",
    "print \"One logistic regression model error: %.2f%%\"%(one_model_test_error_rate*100)\n",
    "# compute the baseline error since the classes are imbalanced\n",
    "print \"Baseline error: %.2f%%\" %((sum(test_labels)*100.)/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Model Averaging\n",
    "Now we'll examine how to combine several models. \n",
    "\n",
    "### 2.1 Overview of Three Classifiers: Decision Tree, k-Nearest Neighbors, and Naive Bayes\n",
    "We can squeeze 'extra robustness' out of our dataset by doing the following simple procedure.  Whereas our original dataset is {}\n",
    "![three_classifiers](../graphics/classifiers_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Averaging Predictions\n",
    "We can squeeze 'extra robustness' out of our dataset by doing the following simple procedure.  Whereas our original dataset is {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(182)\n",
    "\n",
    "# import the three new classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# initialize models\n",
    "d_tree_model = DecisionTreeClassifier()\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "nb_model = GaussianNB()\n",
    "\n",
    "# fit models\n",
    "d_tree_model.fit(train_features, train_labels)\n",
    "knn_model.fit(train_features, train_labels)\n",
    "nb_model.fit(train_features, train_labels)\n",
    "\n",
    "# predict on test data\n",
    "tree_predictions = d_tree_model.predict(test_features)\n",
    "knn_predictions = knn_model.predict(test_features)\n",
    "nb_predictions = nb_model.predict(test_features)\n",
    "\n",
    "# average predictions\n",
    "# add in the logistic regression predictions calcuated previously\n",
    "avg_predictions = np.around((tree_predictions + knn_predictions + nb_predictions + lr_predictions)/4.)\n",
    "\n",
    "print \"Averaging predictions: classification error on test set is %.2f%%\" %(calc_classification_error(avg_predictions, test_labels)*100)\n",
    "print\n",
    "print \"One logistic regression model error: %.2f%%\"%(one_model_test_error_rate*100)\n",
    "# compute the baseline error since the classes are imbalanced\n",
    "print \"Baseline error: %.2f%%\" %((sum(test_labels)*100.)/len(test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Stacking\n",
    "When we performed the averaging above, we used this formula: $$ \\hat f(\\mathbf{x}_{i}) = \\frac{1}{3} \\hat y_{\\text{DT}} + \\frac{1}{3} \\hat y_{\\text{kNN}} + \\frac{1}{3} \\hat y_{\\text{NB}} + \\frac{1}{3} \\hat y_{\\text{LogReg}}.$$  That is, we gave each classifier an equal weighting of 1/3.  While this approach is reasonable, probably it would be better to give an unequal weighting to each classifer.  We can accomplish this by training a second-level logistic regression classifier on the predicted probabilities: $$ \\hat f(\\mathbf{x}_{i}) = \\sigma ( \\alpha_{1} f_{\\text{DT}}(\\mathbf{x}_{i}) + \\alpha_{2}f_{\\text{kNN}}(\\mathbf{x}_{i}) + \\alpha_{3}f_{\\text{NB}}(\\mathbf{x}_{i}) + \\alpha_{4}f_{\\text{LogReg}}(\\mathbf{x}_{i})) \\text{ where } \\sigma (\\cdot) \\text{ is the logistic function}.$$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(182)\n",
    "\n",
    "# calculate probabilities on the training data\n",
    "tree_probs = d_tree_model.predict_proba(train_features)[:,1]\n",
    "knn_probs = knn_model.predict_proba(train_features)[:,1]\n",
    "nb_probs = nb_model.predict_proba(train_features)[:,1]\n",
    "logReg_probs = lr_model.predict_proba(train_features)[:,1]\n",
    "\n",
    "# combine into a new 'feature' matrix\n",
    "prediction_matrix = np.hstack([tree_probs, knn_probs, nb_probs, logReg_probs])\n",
    "\n",
    "# train logistic regression\n",
    "meta_classifier = LogisticRegression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
